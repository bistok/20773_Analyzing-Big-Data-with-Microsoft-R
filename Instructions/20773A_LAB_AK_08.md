# Module 8: Processing Big Data in SQL Server and Hadoop

- [Module 8: Processing Big Data in SQL Server and Hadoop](#module-8-processing-big-data-in-sql-server-and-hadoop)
    - [Lab A: Deploying a predictive model to SQL Server](#lab-a-deploying-a-predictive-model-to-sql-server)
    - [Exercise 1: Upload the flight delay data](#exercise-1-upload-the-flight-delay-data)
        - [Preparation](#preparation)
        - [Task 1: Configure SQL Server and create the FlightDelays database](#task-1-configure-sql-server-and-create-the-flightdelays-database)
        - [Task 2: Upload the flight delay data to SQL Server](#task-2-upload-the-flight-delay-data-to-sql-server)
        - [Task 3: Examine the data in the database](#task-3-examine-the-data-in-the-database)
    - [Exercise 2: Fit a DForest model to the weather delay data](#exercise-2-fit-a-dforest-model-to-the-weather-delay-data)
        - [Task 1: Create a DForest model](#task-1-create-a-dforest-model)
        - [Task 2: Score the DForest model](#task-2-score-the-dforest-model)
    - [Exercise 3: Store the model in SQL Server](#exercise-3-store-the-model-in-sql-server)
        - [Task 1: Save the model to the database](#task-1-save-the-model-to-the-database)
        - [Task 2: Create a stored procedure that runs the model to make predictions](#task-2-create-a-stored-procedure-that-runs-the-model-to-make-predictions)
    - [Lab B: Incorporating Hadoop Map/Reduce and Spark functionality into the ScaleR workflow](#lab-b-incorporating-hadoop-mapreduce-and-spark-functionality-into-the-scaler-workflow)
    - [Exercise 1: Using Pig with ScaleR functions](#exercise-1-using-pig-with-scaler-functions)
        - [Task 1: Examine the Pig script](#task-1-examine-the-pig-script)
        - [Task 2: Upload the Pig script and flight delay data to HDFS](#task-2-upload-the-pig-script-and-flight-delay-data-to-hdfs)
        - [Task 3: Run the Pig script and examine the results](#task-3-run-the-pig-script-and-examine-the-results)
        - [Task 4: Convert the results to XDF format and add field names](#task-4-convert-the-results-to-xdf-format-and-add-field-names)
        - [Task 5: Create graphs to visualize the airline delay data](#task-5-create-graphs-to-visualize-the-airline-delay-data)
        - [Task 6: Investigate the mean airline delay by route](#task-6-investigate-the-mean-airline-delay-by-route)
    - [Exercise 2: Integrating ScaleR code with Spark and Hive](#exercise-2-integrating-scaler-code-with-spark-and-hive)
        - [Task 1: Upload the sorted delay data to Hive](#task-1-upload-the-sorted-delay-data-to-hive)
        - [Task 2: Use Hive to perform ad-hoc queries over the data](#task-2-use-hive-to-perform-ad-hoc-queries-over-the-data)
        - [Task 3: Use a sparklyr session to analyze data](#task-3-use-a-sparklyr-session-to-analyze-data)

## Lab A: Deploying a predictive model to SQL Server

## Exercise 1: Upload the flight delay data

### Preparation

1. Ensure that the **MT17B-WS2016-NAT**, **20773A-LON-DC**, **20773A-LON-DEV**, **20773A-LON-RSVR**, and **20773A-LON-SQLR** virtual machines are running
2. Log on to the **LON-SQLR** VM as **Adatum\\AdatumAdmin** with the password **Pa55w.rd**.
3. Using Internet Explorer, go to the URL **https://www.microsoft.com/download/details.aspx?id=54513**
4. On the **Important Update for SQL Server 2016 SP1** page, click **Download**.
5. In the Internet Explorer message bar, click **Run**.
6. In the **Install a SQL Server 2016 Update** wizard, on the **License Terms** page, check **I accept the license terms**, and then click **Next**.
7. On the **Select Features** page, click **Next**.
8. On the **Consent to install Microsoft R Open** page, click **Accept**, and then click **Next**.
9. On the **Check Files In Use** page, click **Next**.
10. On the **Ready to update** page, click **Update**.
11. On the **Complete** page, click **Close**.
12. Close Internet Explorer.

### Task 1: Configure SQL Server and create the FlightDelays database

1. Log on to the **LON-DEV** VM as **Adatum\\AdatumAdmin** with the password **Pa55w.rd**.
2. On the Windows desktop, click **Start**, type **Microsoft SQL Server Management Studio**, and then press Enter.
3. In the **Connect to Server** dialog box, log on to **LON-SQLR** using Windows authentication.
4. In the toolbar, click **New Query**.
5. In the **Query** window, type the following commands. These commands enable you to run the **sp\_execute\_external\_script** stored procedure:

    ````SQL
    sp_configure 'external scripts enabled', 1;
    RECONFIGURE;
    ````

6. In the toolbar, click **Execute**.
7. In Object Explorer, right-click **LON-SQLR**, and then click **Restart**.
8. In the **Microsoft SQL Server Management Studio** message box, click **Yes**.
9. In the second **Microsoft SQL Server Management Studio** message box, click **Yes**.
10. Wait for SQL Server to restart.
11. In Object Explorer, expand **LON-SQLR**, right-click **Databases**, and then click **New Database**.
12. In the **New Database** dialog box, in the **Database name** text box, type **FlightDelays**, and then click **OK**.
13. Leave SQL Server Management Studio open.

### Task 2: Upload the flight delay data to SQL Server

1. Open a command prompt window.
2. Run the following commands:

    ````CMD
    
    copy E:\Labfiles\Lab08\FlightDelayDataSample.xdf E:\Data
    ````

3. Verify that the file is copied successfully, and then close the command prompt.
4. Start your R development environment of choice (Visual Studio 2015, or RStudio), and create a new R file.
5. In the script editor, add the following statement to the R file and run it. This code ensures that you are running in the local compute context:

    ````R
    rxSetComputeContext(RxLocalSeq())
    ````

6. In the script editor, add the following statements to the R file and run them. This code creates a connection string for the SQL Server **FlightDelays** database, and an **RxSqlServerData** data source for the **flightdelaydata** table in the database:

    ````R
    connStr <- "Driver=SQL Server;Server=LON-SQLR;Database=FlightDelays;Trusted_Connection=Yes"
    flightDelayDataTable <- RxSqlServerData(connectionString = connStr, table = "flightdelaydata")
    ````

7. Add the following code to the R file and run it. These statements import the data from the **FlightDelayDataSample.xdf** file and adds the **DelayedByWeather** logical factor and **Dataset** column to each observation:

    ````R
    rxOptions("reportProgress" = 2)
    flightDelayDataFile <- "E:\\Data\\FlightDelayDataSample.xdf"
    flightDelayData <- rxDataStep(inData = flightDelayDataFile,
        outFile = flightDelayDataTable, overwrite = TRUE,
        transforms = list(DelayedByWeather = factor(ifelse(is.na(WeatherDelay), 0, WeatherDelay) > 0, levels = c(FALSE, TRUE)),
        Dataset = factor(ifelse(runif(.rxNumRows) >= 0.05, "train", "test")))
    )
    ````

### Task 3: Examine the data in the database

1. Add the following statements to the R file and run them. This code creates a new SQL Server compute context:

    ````R
    sqlWait <- TRUE
    sqlConsoleOutput <- TRUE
    sqlContext <- RxInSqlServer(connectionString = connStr,
        wait = sqlWait, consoleOutput = sqlConsoleOutput
    )
    rxSetComputeContext(sqlContext)
    ````

2. Add the following code to the R file and run it. These statements create an **RxSqlServerData** data source that reads the flight delay data from the SQL Server database and refactors it:

    ````R
    weatherDelayQuery = "SELECT Month, MonthName, OriginState, DestState,
        Dataset, DelayedByWeather, WeatherDelayCategory = CASE
            WHEN CEILING(WeatherDelay) <= 0 THEN 'No delay'
            WHEN CEILING(WeatherDelay) BETWEEN 1 AND 30 THEN '1-30 minutes'
            WHEN CEILING(WeatherDelay) BETWEEN 31 AND 60 THEN '31-60 minutes'
            WHEN CEILING(WeatherDelay) BETWEEN 61 AND 120 THEN '61-120 minutes'
            WHEN CEILING(WeatherDelay) BETWEEN 121 AND 180 THEN '121-180 minutes'
            WHEN CEILING(WeatherDelay) >= 181 THEN 'More than 180 minutes'
        END
        FROM flightdelaydata"

    delayDataSource <- RxSqlServerData(sqlQuery = weatherDelayQuery,
        colClasses = c(
            Month = "factor",
            OriginState = "factor",
            DestState = "factor",
            Dataset = "character",
            DelayedByWeather = "factor",
            WeatherDelayCategory = "factor"
        ),
        colInfo = list(
            MonthName = list(
                type = "factor",
                levels = month.name
            )
        ),
        connectionString = connStr
    )
    ````

3. Add the following statement to the R file and run it. This statement retrieves the details for each variable in the data source:

    ````R
    rxGetVarInfo(delayDataSource)
    ````

    There should be seven variables, named **Month**, **MonthName**, **OriginState**, **DestState**, **Dataset**, **DelayedByWeather**, and **WeatherDelayCategory**.

4. Add the following statement to the R file and run it. This statement retrieves the data from the data source and summarizes it:

    ````R
    rxSummary(~., delayDataSource)
    ````

5. Add the following code to the R file and run it. This statement creates a histogram that shows the categorized delays by month:

    ````R
    rxHistogram(~WeatherDelayCategory | MonthName,
        data = delayDataSource,
        xTitle = "Weather Delay",
        scales = (list(
            x = list(rot = 90)
        ))
    )
    ````

6. Add the following code to the R file and run it. This statement creates a histogram that shows the categorized delays by origin state:

    ````R
    rxHistogram(~WeatherDelayCategory | OriginState,
        data = delayDataSource,
        xTitle = "Weather Delay",
        scales = (list(
            x = list(rot = 90, cex = 0.5)
        ))
    )
    ````

**Results**: At the end of this exercise, you will have imported the flight delay data to SQL Server and used ScaleR functions to examine this data.

## Exercise 2: Fit a DForest model to the weather delay data

### Task 1: Create a DForest model

1. Add the following code to the R file and run it. This statement fits a DTree model to the weather delay data:

    ````R
    weatherDelayModel <- rxDForest(DelayedByWeather ~ Month + OriginState + DestState,
        data = delayDataSource,
        cp = 0.0001,
        rowSelection = (Dataset == "train")
    )
    ````

2. Add the following statement to the R file and run it. This statement summarizes the forecast accuracy of the model:

    ````R
    print(weatherDelayModel)
    ````

3. Add the following statement to the R file and run it. This statement shows the structure of the decision trees in the DForest model:

    ````R
    head(weatherDelayModel)
    ````

4. Add the following statement to the R file and run it. This statement shows the relative importance of each predictor variable to the decisions made by the model:

    ````R
    rxVarUsed (weatherDelayModel)
    ````

### Task 2: Score the DForest model

1. Add the following code to the R file and run it. This statement modifies the query used by the data source and adds a WHERE clause to limit the rows retrieved to the test data:

    ````R
    delayDataSource@sqlQuery <- paste(delayDataSource@sqlQuery, "WHERE Dataset = 'test'", sep = " ")
    ````

2. Add the following code to the R file and run it. This statement creates a data source that will be used to store scored results in the database:

    ````R
    weatherDelayScoredResults <- RxSqlServerData(connectionString = connStr, table = "scoredresults")
    ````

3. Add the following code to the R file and run it. These statements switch to the local compute context, generate weather delay predictions using the new data set and save the scored results in the **scoredresults** table in the database, and then return to the SQL Server compute context:

    ````R
    rxSetComputeContext(RxLocalSeq())
    rxPredict(modelObj = weatherDelayModel,
        data = delayDataSource,
        outData = weatherDelayScoredResults, overwrite = TRUE,
        writeModelVars = TRUE,
        predVarNames = c("PredictedDelay", "PredictedNoDelay", "PredictedDelayedByWeather"),
        type = "prob"
    )
    rxSetComputeContext(sqlContext)
    ````

4. Add the following code to the R file and run it. This code tests the scored results against the real data and plots the accuracy of the predictions:

    ````R
    install.packages('ROCR')
    library(ROCR)

    # Transform the prediction data into a standardized form
    results <- rxImport(weatherDelayScoredResults)
    weatherDelayPredictions <- prediction(results$PredictedDelay, results$DelayedByWeather)

    # Plot the ROC curve of the predictions
    rocCurve <- performance(weatherDelayPredictions, measure = "tpr", x.measure = "fpr")
    plot(rocCurve)
    ````

5. If the **Microsoft Visual Studio** dialog boxes appear, click **Yes**.

**Results**: At the end of this exercise, you will have created a decision tree forest using the weather data held in the SQL Server database, scored it, and stored the results back in the database.

## Exercise 3: Store the model in SQL Server

### Task 1: Save the model to the database

1. Add the following code to the R file and run it. These statements created a serialized representation of the model as a string of binary data:

    ````R
    serializedModel <- serialize(weatherDelayModel, NULL)
    serializedModelString <- paste(serializedModel, collapse = "")
    ````

2. Return to SQL Server Management Studio.

3. In the toolbar, click **New Query**. In the Query window, type the following code:

    ````SQL
    USE FlightDelays;
    CREATE TABLE [dbo].[delaymodels]
    (
        modelId INT IDENTITY(1,1) NOT NULL Primary KEY,
        model VARBINARY(MAX) NOT NULL
    );
    ````

4. In the toolbar, click **Execute**. Verify that the code runs without any errors.
5. Overwrite the code in the **Query** window with the following block of Transact-SQL:

    ````SQL
    CREATE PROCEDURE [dbo].[PersistModel] @m NVARCHAR(MAX)
    AS
    BEGIN
        SET NOCOUNT ON;
        INSERT INTO delaymodels(model) VALUES (CONVERT(VARBINARY(MAX),@m,2))
    END;
    ````

6. In the toolbar, click **Execute**. Verify that the code runs without any errors.
7. Return to your R development environment.
8. Add the following statements to the R file and run it. This code uses an ODBC connection to run the **PersistModel** stored procedure and save your DTree model to the database:

    ````R
    install.packages('RODBC')
    library(RODBC)
    connection <- odbcDriverConnect(connStr)
    cmd <- paste("EXEC PersistModel @m='", serializedModelString, "'", sep = "")
    sqlQuery(connection, cmd)
    ````

### Task 2: Create a stored procedure that runs the model to make predictions

1. Switch back to SQL Server Management Studio.
2. Overwrite the code in the **Query** window with the following block of Transact-SQL:

    ````SQL
    CREATE PROCEDURE [dbo].[PredictWeatherDelay]
    @Month integer = 1,
    @OriginState char(2),
    @DestState char(2)
    AS
    BEGIN
        DECLARE @weatherDelayModel varbinary(max) = (SELECT TOP 1 model FROM dbo.delaymodels)
        EXEC sp_execute_external_script @language = N'R', @script = N'
            delayParams <- data.frame(Month = month, OriginState = originState, DestState = destState)
            delayModel <- unserialize(as.raw(model))
            OutputDataSet<-rxPredict(modelObject = delayModel,
            data = delayParams,
            outData = NULL,
            predVarNames = c("PredictedDelay", "PredictedNoDelay", "PredictedDelayedByWeather"),
            type = "prob",
            writeModelVars = TRUE)',
        @params = N'@model varbinary(max),
            @month integer,
            @originState char(2),
            @destState char(2)',
            @model = @weatherDelayModel,
            @month = @Month,
            @originState = @OriginState,
            @destState = @DestState
        WITH RESULT SETS (([PredictedDelay] float, [PredictedNoDelay] float, [PredictedDelayedByWeather] bit, [Month] integer, [OriginState] char(2), [DestState] char(2)));
    END
    ````

3. In the toolbar, click **Execute**. Verify that the code runs without any errors.
4. Return to your R development environment.
5. Add the following statements to the R file and run it. This code tests the stored procedure:

    ```` R
    cmd <- "EXEC [dbo].[PredictWeatherDelay] @Month = 10, @OriginState = 'MI', @DestState = 'NY'"
    sqlQuery(connection, cmd)
    ````

6. Save the script as **Lab8\_1Script.R** in the **E:\\Labfiles\\Lab08** folder, and close your R development environment.
7. Close SQL Server Management Studio, without saving any changes.

**Results**: At the end of this exercise, you will have saved the DForest model to SQL Server, and created a stored procedure that you can use to make weather delay predictions using this model.

## Lab B: Incorporating Hadoop Map/Reduce and Spark functionality into the ScaleR workflow

## Exercise 1: Using Pig with ScaleR functions

### Preparation

1. Ensure that the **MT17B-WS2016-NAT**, **20773A-LON-DC**, **20773A-LON-DEV**, **20773A-LON-RSVR**, and **20773A-LON-SQLR** virtual machines are running.
2. Follow the instructions in the document **Creating the Hadoop Cluster on Azure** to deploy and configure an Azure HDInsight cluster.
3. In the Azure portal, go to the **LON-HADOOP-SERVER** resource group, and click the **LON-HADOOP-*nn*** HDInisght cluster.
4. On the **LON-HADOOP-*nn*** blade, in the **R Server dashboards** section, click **Ambari home**.
5. In the **Sign in** dialog box, sign in as **admin** with password **Pa55w.rdPa55word**.
6. On the **Ambari** page, in the left pane, click **Tez**.
7. Click the **Configs** tab.
8. Expand the **Advanced tez-site** section.
9. Change the value of the **tez.runtime.io.sort.mb** configuration setting to **1024 MB**.
10. Click **Save**.
11. In the **Save Configuration** dialog box, click **Save**.
12. In the **Configurations** dialog box, click **Proceed Anyway**.
13. In the **Save Configuration Changes** dialog box, click **OK**.
14. Click **Restart**, and then click **Restart All Affected**.
15. In the **Confirmation** dialog box, click **Confirm Restart All**.
16. Wait for the Tez components to restart, and then click **OK**.

### Task 1: Examine the Pig script

1. Log on to the **LON-DEV** VM as **Adatum\\AdatumAdmin** with the password **Pa55w.rd**.
2. Using **WordPad**, open the file **carrierDelays.pig** in the **E:\\Labfiles\\Lab08** folder.
3. Review the script, and close Wordpad.

### Task 2: Upload the Pig script and flight delay data to HDFS

1. Using **File Explorer**, go to the folder **E:\\Labfiles\\Lab08**, right-click the file **FlightDelayDataSample.zip**, and then click **Extract All**.
2. In the **Extract Compressed (Zipped) Folders** dialog box, specify the folder **E:\\Labfiles\\Lab08**, and then click **Extract**.
3. Start your R development environment of choice (Visual Studio 2015, or RStudio), and create a new R file.
4. In the script editor, add the following statements to the R file and run them. These statements establish a new **RxHadoopMR** compute context:

    ````R
    loginName = "sshuser"
    context <- RxHadoopMR(sshUsername = loginName,
        sshHostname = "LON-HADOOP",
        consoleOutput = TRUE)
    rxSetComputeContext(context, wait = TRUE)
    ````

5. Add the following statements to the R file and run them. This code removes the **FlightDelayDataSample.csv** file from your directory in HDFS (if it exists; you can ignore the error message if it is not found), and copies the latest data from the **E:\\Labfiles\\Lab08** folder:

    ````R
    rxHadoopRemove(path = paste("/user/RevoShare/", loginName, "/FlightDelayDataSample.csv", sep=""))
    rxHadoopCopyFromClient(source = "E:\\Labfiles\\Lab08\\FlightDelayDataSample.csv",
        hdfsDest = paste("/user/RevoShare/", loginName, sep=""))
    ````

6. Add the following statements to the R file and run them. This code uploads the **carriers.csv** file to your directory in HDFS. Note that this is a large file, and it will take a couple of minutes to upload:

    ````R
    rxHadoopRemove(path = paste("/user/RevoShare/", loginName, "/carriers.csv", sep=""))
    rxHadoopCopyFromClient(source = "E:\\Labfiles\\Lab08\\carriers.csv",
        hdfsDest = paste("/user/RevoShare/", loginName, sep=""))
    ````

7. Switch to the Azure portal.
8. On the navigation blade on the left side of the portal, click **Resource groups**.
9. Click the **LON-HADOOP-SERVER** resource group.
10. On the **LON-HADOOP-SERVER** blade, click the **ipConfig1** Public IP address.
11. On the **ipConfig1** blade, make a note of the IP address.
12. Return to your R development environment.
13. Add the following statements to the R file and run them. Replace the text **ipaddress** with value of the **ipConfig1** public IP address for the edge node of the cluster. This code closes the **RxHadoopMR** compute context, creates a remote R session on the Hadoop cluster, and loads the **RevoScaleR** library in that session:

    ````R
    rxSetComputeContext(RxLocalSeq())
    remoteLogin(deployr_endpoint = "http://<ipaddress>:12800", session = TRUE, diff = TRUE, commandline = TRUE,      username = "admin", password = "Pa55w.rd")
    library(RevoScaleR)
    ````

14. Add the following statements to the R file and run them. This code copies the Pig script (and the **loginName** variable) to the remote session:

    ````R
    pause()
    putLocalFile("E:\\Labfiles\\Lab08\\carrierDelays.pig")
    putLocalObject(c("loginName"))
    resume()
    ````

### Task 3: Run the Pig script and examine the results

1. Add the following statement to the R file and run it. This code runs the Pig script:

   ````R
    result <- system("pig carrierDelays.pig", intern = TRUE)
    ````

2. Add the following code to the R file and run it. This code lists the contents of the /user/RevoShare/sshuser directory in HDFS. This directory should include a subdirectory named **results**:

    ````R
    rxHadoopCommand(paste("fs -ls -R /user/RevoShare/", loginName, sep = ""), intern = TRUE)
    ````

3. Verify that the results directory contains two files: **\_SUCCESS** and **part-r-00000**. The data is actually in the part-r-00000 file.
4. Add the following statement to the R file and run it. This code deletes the **\_SUCCESS** file from the **results** directory:

    ````R
    rxHadoopRemove(paste("fs -ls -R /user/RevoShare/", loginName, "/results/_SUCCESS", sep = ""))
    ````

5. Add the following statements to the R file and run them. These statements display the structure of the results generated by the Pig script:

    ````R
    rxOptions(reportProgress = 1)
    rxSetFileSystem(RxHdfsFileSystem())
    resultsFile <- paste("/user/RevoShare/", loginName, "/results", sep = "")
    resultsData <- RxTextData(resultsFile)
    rxGetVarInfo(resultsData)
    ````

### Task 4: Convert the results to XDF format and add field names

1. Add the following code to the R file and run it:

    ````R
    carrierDelayColInfo <- list(V1 = list(type = "factor", newName = "Origin"),
        V2 = list(type = "factor", newName = "Dest"),
        V3 = list(type = "factor", newName = "AirlineCode"),
        V4 = list(type = "character", newName = "AirlineName"),
        V5 = list(type = "numeric", newName = "CarrierDelay"),
        V6 = list(type = "numeric", newName = "LateAircraftDelay")
    )
    ````

2. Add the following code to the R file and run it. This code creates the **CarrierData** composite XDF file in HDFS:

    ````R
    carrierFile <- RxXdfData(paste("/user/RevoShare", loginName, "CarrierData", sep = "/"))
    carrierData <- rxImport(inData = resultsData,
        outFile = carrierFile, overwrite = TRUE,
        colInfo = carrierDelayColInfo,
        createCompositeSet = TRUE
    )
    ````

3. Add the following code to the R file and run it. This code creates a new **RxHadoopMR** compute context in the remote session:

    ````R
    hadoopContext = RxHadoopMR(sshUsername = loginName, consoleOutput = TRUE)
    rxSetComputeContext(hadoopContext)
    ````

4. Add the following code to the R file and run it. This code displays the structure of the XDF file which should now include the new mappings:

    ````R
    rxGetVarInfo(carrierData)
    ````

5. Add the following code to the R file and run it. This code summarizes the contents of the XDF file:

    ````R
    rxSummary(~., carrierData)
    ````

    > **Note:** This function runs as a Map/Reduce job

### Task 5: Create graphs to visualize the airline delay data

1. Add the following code to the R file and run it. This code creates a data frame that contains the airline delay information (carrier delay plus late aircraft delay), and then uses this data frame to generate a histogram of delay times:

    ````R
    transformedCarrierData <- rxDataStep(carrierData, transforms = list(TotalDelay = CarrierDelay + LateAircraftDelay))
    # The data source for this histogram is an in-memory data frame, so the processing is not distributed
    rxHistogram(~TotalDelay, data = transformedCarrierData,
        xTitle = "Carrier + Late Aircraft Delay (minutes)",
        yTitle = "Occurrences",
        endVal = 300
    )
    ````

2. Add the following code to the R file and run it. This code changes the resolution of the plot window to 1024 by 768 pixels, and then generates a histogram showing the number of delayed flights for each airline:

    ````R
    png(width=1024, height=768);rxHistogram(~AirlineCode, data = carrierData,
        xTitle = "Airline",
        yTitle = "Number of delayed flights"
    );dev.off()
    ````

3. Add the following code to the R file and run it. This code creates a bar chart showing the total delay time for all flights made by each airline:

    ````R
    install.packages("ggplot2")
    library(ggplot2)
    ggplot(data = rxImport(carrierData, transforms = list(TotalDelay = CarrierDelay + LateAircraftDelay))) +
        geom_bar(mapping = aes(x = AirlineName, y = TotalDelay), stat = "identity") +
        labs(x = "Airline", y = "Total Carrier + Late Aircraft Delay (minutes)") +
        scale_x_discrete(labels = function(x) { lapply(strwrap(x, width = 25, simplify = FALSE), paste, collapse = "\n")}) +
        theme(axis.text.x = element_text(angle = 90, size = 8))
    ````

### Task 6: Investigate the mean airline delay by route

1. Add the following code to the R script and run it. This code creates a data cube that calculates the mean delay for each airline by route:

    ````R
    delayData <- rxCube(AverageDelay ~ AirlineCode:Origin:Dest, carrierData,
        transforms = list(AverageDelay = CarrierDelay + LateAircraftDelay),
        means = TRUE,
        na.rm = TRUE,
        removeZeroCounts = TRUE
    )
    ````

2. Add the following code to the R script and run it. This code uses the **rxExec** function to run **rxSort** to sort the data in the cube:

    ````R
    sortedDelayData <- rxExec(FUN=rxSort, as.data.frame(delayData), sortByVars = c("Counts", "AverageDelay"), decreasing = c(TRUE, TRUE), timesToRun = 1)
    ````

3. Add the following code to the R script and run it. This the top 50 rows in the sorted cube (the routes with the most frequent and longest delays).

    ````R
    head(sortedDelayData$rxElem1, 50)
    ````

4. Add the following code to the R script and run it. This code shows the bottom 50 rows in the sorted cube (the routes with the least frequent and shortest delays):

    ````R
    tail(sortedDelayData$rxElem1, 50)
    ````

5. Add the following code to the R script and run it. This code switches to the local compute context and saves the data cube to the file SortedDelayData.csv in HDFS:

    ````R
    rxSetComputeContext(RxLocalSeq())
    sortedDelayDataFile <- paste("/user/RevoShare/", loginName, "/SortedDelayData.csv", sep = "")
    sortedDelayDataCsv <- RxTextData(sortedDelayDataFile)
    sortedDelayDataSet <- rxDataStep(inData = sortedDelayData$rxElem1,
        outFile = sortedDelayDataCsv, overwrite = TRUE
    )
    ````

**Results**: At the end of this exercise, you will have run a Pig script from R, and analyzed the data that the script produces.

## Exercise 2: Integrating ScaleR code with Spark and Hive

### Task 1: Upload the sorted delay data to Hive

1. Add the following code to the R script and run it. This code creates an **RxSpark** compute context:

    ````R
    sparkContext = RxSpark(sshUsername = loginName,
        consoleOutput = TRUE,
        numExecutors = 10,
        executorCores = 2,
        executorMem = "1g",
        driverMem = "1g"
    )
    rxSetComputeContext(sparkContext)
    ````

2. Add the following code to the R script and run it. This code creates an **RxHiveData** data source:

    ````R
    dbTable <- "RouteDelays"
    hiveDataSource <- RxHiveData(table = dbTable)
    ````

3. Add the following code to the R script and run it. This code uploads the data to Hive:

    ````R
    data <- rxDataStep(inData = sortedDelayDataSet,
        outFile = hiveDataSource, overwrite = TRUE)
    ````

4. Add the following code to the R script and run it. This code runs the **rxSummary** function over the Hive data:

    ````R
    rxSummary(~., hiveDataSource)
    ````

### Task 2: Use Hive to perform ad-hoc queries over the data

1. Open a command prompt window, and run the **putty** command.
2. In the **PuTTY Configuration** window, select the **LON-HADOOP** session, click **Load**, and then click **Open**. You should be logged in to the Hadoop VM.
3. In the PuTTY terminal window, run the following command:

    ````Bash
    /usr/bin/hive
    ````

4. At the **hive\>** prompt, run the following command:

    ````SQL
    select * from routedelays;
    ````

    Verify that 8852 rows are retrieved.

5. At the **hive\>** prompt, run the following command:

    ````SQL
    select count(*), airlinecode from routedelays group by airlinecode;
    ````

    This command lists each airline together with the number of delayed flights for that airline.

6. At the **hive\>** prompt, run the following command to exit hive:

    ````SQL
    exit;
    ````

7. Close the PuTTY terminal window, and return to your R development environment.
8. Save the script as **Lab8\_2Script.R** in the **E:\\Labfiles\\Lab08** folder, and close your R development environment.

**Results**: At the end of this exercise, you will have used R code running in an RxSpark compute context to upload data to Hive, and then analyzed the data by performing Hive queries.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
