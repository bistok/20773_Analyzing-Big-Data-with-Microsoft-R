# Module 8: Processing Big Data in SQL Server and Hadoop

- [Module 8: Processing Big Data in SQL Server and Hadoop](#module-8-processing-big-data-in-sql-server-and-hadoop)
    - [Demo 1: Storing and Retrieving R Objects from a Database](#demo-1-storing-and-retrieving-r-objects-from-a-database)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Save an R Object to SQL Server](#save-an-r-object-to-sql-server)
        - [Retrieve an R Object from SQL Server](#retrieve-an-r-object-from-sql-server)
    - [Demo 2: Running an Analysis as a Map/Reduce Job](#demo-2-running-an-analysis-as-a-mapreduce-job)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Upload data to HDFS in the Hadoop compute context](#upload-data-to-hdfs-in-the-hadoop-compute-context)
        - [Analyze the data in Hadoop](#analyze-the-data-in-hadoop)
        - [Track Map/Reduce jobs](#track-mapreduce-jobs)

## Demo 1: Storing and Retrieving R Objects from a Database

### Scenario

This demonstration shows how to save an R object to a SQL Server database, and then retrieve it later.

In this demonstration, you will see how to:

- Save an R object to SQL Server.
- Retrieve an R object from SQL Server.

### Preparation

1. Verify that the VMs are running.
2. Log in to the  VM as **.\student** with the password **Pa55w.rd**.
3. Click **Start**, type **Microsoft SQL Server Management Studio**, and then press Enter.
4. In the **Connect to Server** dialog box, log in to **(local)** using **Windows authentication**.
5. In the toolbar, click **New Query**.
6. In the **Query** window, type the following commands. These commands enable you to run the **sp\_execute\_external\_script** stored procedure:

    ```SQL
    sp_configure 'external scripts enabled', 1;
    RECONFIGURE;
    ```

7. In the toolbar, click **Execute**.
8. In **Object Explorer**, right-click **(local)**, and then click **Restart**.
9. In the **Microsoft SQL Server Management Studio** message box, click **Yes**.
10. In the second **Microsoft SQL Server Management Studio** message box, click **Yes**.
11. Wait for SQL Server to restart.
12. In **Object Explorer**, right-click **Databases**, and then click **New Database**.
13. In the **New Database** dialog box, in the **Database name** box, type **AirlineData**, and then click **OK**.
14. Close SQL Server Management Studio, without saving any chnages.
15. Start your R development environment of choice (Visual StudioÂ® or RStudio).
16. Open the R script **E:\\Demofiles\\Mod08\\importAirports.R**.
17. Run the script.
18. Close the script.

### Save an R Object to SQL Server

1. In your R development environment, open the R script **Demo1 - persisting R objects.R** in the **E:\\Demofiles\\Mod08** folder.
2. Highlight and run the code under the comment **# Create a SQL Server compute context**. This code creates a compute context that connects to the AirlineData database.
3. Highlight and run the code under the comment **# Create a data source that retrieves airport information**.
4. Highlight and run the code under the comment **# Create and display a histogram of airports by state**. This code uses the rxHistogram function to generate the histogram. Notice that the histogram object itself is stored in the chart variable.

    > **Note**: Make sure that you display the Plots window in the lower right pane.

5. In the toolbar above the **Plots** window, click **Clear all Plots**, and then click **Yes** to confirm.
6. On the Windows desktop, click **Start**, type **Microsoft SQL Server Management Studio**, and then press Enter.
7. In the **Connect to Server** dialog box, log in to **(local)** using **Windows authentication**.
8. On the **File** menu, point to **Open** and then click **File**.
9. Move to the **E:\\Demofiles\\Mod08** folder, click the **Demo1 - persisting R objects.sql** SQL Server query file, and then click **Open**. This script creates a table named **charts** with two columns; **id** and **value**. This table will be used to hold R objects. The id column is the primary key, and the value column will hold a serialized version of the object.
10. In the toolbar, click **Execute**.
11. Return to your R development environment.
12. Highlight and run the code under the comment **# Create an ODBC data source that connects to the charts table**. This code switches back to the local compute context and creates an ODBC data source.
13. Highlight and run the code under the comment **# Save the chart to the charts table, and give it a unique name to identify it later**. This code uses the **rxWriteObject** function to store the histogram object with the key **chart1**.
14. Switch back to SQL Server Management Studio.
15. In Object Explorer, expand **(local)**, expand **Databases**, expand **AirlineData**, right-click **Tables**, and then click **Refresh**.
16. Expand **Tables**, right-click **dbo.charts**, and then click **Select Top 1000 Rows**. You should see a single row. The value is a hexadecimal string that is a binary representation of the histogram object.

### Retrieve an R Object from SQL Server

1. Return to your R development environment.
2. Highlight and run the code under the comment **# Retrieve the persisted chart**. This code uses the **rxReadObject** function to read the data for the chart1 object from the database and reinstate it as an R object in memory.
3. Highlight and run the code under the comment **# Display the chart**. This code prints the object. The histogram should appear in the **Plots** window.
4. Close your R development environment without saving any changes.

## Demo 2: Running an Analysis as a Map/Reduce Job

### Scenario

This demonstration shows how to use the RxHadoopMR compute context to analyze data using ScaleR functions.

In this demonstration, you will see how to:

- Upload data to HDFS in the Hadoop compute context
- Analyze data in Hadoop
- Track Map/Reduce jobs>

### Preparation

1. Ensure that an Azure HDInsight cluster is running, setup and configured as described by the document **Creating the Hadoop Cluster on Azure**.
2. Verify that the, **20773A-LON-DC**, **20773A-LON-DEV**, and **MT17B-WS2016-NAT** VMs are running.
3. Log in to the **LON-DEV** VM as **AdatumAdmin** with the password **Pa55w.rd**.

### Upload data to HDFS in the Hadoop compute context

1. Open your R development environment of choice (RStudio or Visual Studio).
2. Open the R script **Demo2 - analysis on Hadoop.R** in the **E:\\Demofiles\\Mod08** folder.
3. Highlight and run the code under the comment **# Create a Hadoop compute context**. These statements connect to the **LON-HADOOP** server as instructor. Note that the host name is actually the name of a PuTTY configuration file (LON-HADOOP) and not the name of the remote server. The PuTTY configuration file contains the details and keys required to establish an SSH session on the server.
4. Highlight and run the code under the comment **# Copy FlightDelayData.xdf to HDFS (/user/RevoShare/sshuser)**. These statements remove the FlightDelayData.xdf file from HDFS (if it exists), and then copy a new version of this file from the E:\Demofiles\Mod08 folder on the client VM.

    If the **rxHadoopRemove** command reports No such file or directory then the file didn't exist. You can ignore this message.

5. Highlight and run the code under the comment **# Verify that the file has been uploaded**. This statement uses the rxHadoopCommand function to display the contents of the /user/RevoShare/sshuser folder in HDFS. You should see the FlightDelayData.xdf file included in the output.

### Analyze the data in Hadoop

1. Highlight and run the code under the comment **# Connect to the R server running Hadoop**, replacing the *nn* elements in the Hadoop URL with the URL of the Hadoop cluster in Azure. This code creates a remote session connecting to R Server on the Hadoop machine.
2. Highlight and run the code under the comment **# Create a Hadoop compute context on this server**. Note that you don't have to specify the host name this time because Hadoop is running on the same machine as your session.
3. Highlight and run the code under the comment **# Examine the structure of the data**. This code runs the **rxGetVarInfo** function to view the variables in the data file. Notice that you have to switch to the HDFS file system; the default file system for the Hadoop compute context is the native Linux file system. Additionally, you will see a few messages reported by the compute context before the results are displayed. This is because the consoleOutput flag in the compute context is set to TRUE. In a production environment, you would typically disable this feature, but it is useful for debugging in a test and development environment.
4. Highlight and run the code under the comment **# Read a subset of the data into a data frame**. This code runs the **rxImport** function to fetch a sample of 10 percent of the data into memory.
5. Highlight and run the code under the comment **# Summarize the flight delay data sample**. This code runs the **rxSummary** function over the data frame. Notice that Hadoop displays the message **Warning: Computations on data sets in memory cannot be distributed. Computations are being done on a single node.** The task is performed as a single-threaded operation rather than a Map/Reduce job.
6. Highlight and run the code under the comment **# Break the data file down into composite pieces**. The ScaleR functions in the Hadoop Map/Reduce compute context are optimized to work with composite files. This code stores the composite version of the data file in the /user/RevoShare/instructor/DelayData directory in HDFS. This directory must exist before creating the file, so this code creates it using the **rxHadoopMakeDir** function.
7. Highlight and run the code under the comment **# Examine the composite XDF file**. This block of code uses the rxHadoopListFiles function to display the contents of the DelayData folder and data subfolder.
8. Highlight and run the code under the comment **# Perform a more complex analysis - compute a crosstab**. This code generates a crosstab of airlines and the airports that they serve, counting the number of flights that have departed from each airport for that airline.

### Track Map/Reduce jobs

1. On the desktop, open **Microsoft Edge**.
2. Navigate to the Azure portal, **http://portal.azure.com** and log in.
3. In the left-hand pane, Click **Resource groups**.
4. In the **Resource groups** blade, click **LON-HADOOP-SERVER**.
5. In the **LON-HADOOP-SERVER** blade, click the **LON-HADOOP-nn HDInsight** cluster.
6. In the **LON-HADOOP-nn** blade, click **R Server dashboards**.
7. In the **Cluster dashboards** blade, click **Yarn**. 
8. In the **Windows Security** dialog box, enter **Admin** with the **Pa55w.rdPa55w.rd** password, and then click **OK**.
9. The Hadoop Job Tracking page should appear, showing the details of each Map/Reduce job that you have performed during the demonstration.
10. In the **ID** column, click the link for the most recent job. This should be the job that ran when you created the crosstab. The details for the job should appear on a new page.
11. In the bottom pane, click the **Logs** link. This page shows the trace for the job. This information is useful for debugging purposes, if a ScaleR function fails for some reason.
12. Click the **Back** button in the toolbar to return to the previous page, and then click the **Back** button again to return to the Job Tracking page listing all the recent jobs. If time allows, examine the other jobs.
13. Close Microsoft Edge.
14. Return to your R development environment.
15. Close your R development environment without saving changes.

---

Â©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
