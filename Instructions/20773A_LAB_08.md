# Module 8: Processing Big Data in SQL Server and Hadoop

- [Module 8: Processing Big Data in SQL Server and Hadoop](#module-8-processing-big-data-in-sql-server-and-hadoop)
    - [Lab A: Deploying a predictive model to SQL Server](#lab-a-deploying-a-predictive-model-to-sql-server)
        - [Scenario](#scenario)
        - [Objectives](#objectives)
        - [Lab Setup](#lab-setup)
    - [Exercise 1: Upload the flight delay data](#exercise-1-upload-the-flight-delay-data)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Task 1: Configure SQL Server and create the FlightDelays database](#task-1-configure-sql-server-and-create-the-flightdelays-database)
        - [Task 2: Upload the flight delay data to SQL Server](#task-2-upload-the-flight-delay-data-to-sql-server)
        - [Task 3: Examine the data in the database](#task-3-examine-the-data-in-the-database)
    - [Exercise 2: Fit a DForest model to the weather delay data](#exercise-2-fit-a-dforest-model-to-the-weather-delay-data)
        - [Scenario](#scenario)
        - [Task 1: Create a DForest model](#task-1-create-a-dforest-model)
        - [Task 2: Score the DForest model](#task-2-score-the-dforest-model)
    - [Exercise 3: Store the model in SQL Server](#exercise-3-store-the-model-in-sql-server)
        - [Scenario](#scenario)
        - [Task 1: Save the model to the database](#task-1-save-the-model-to-the-database)
        - [Task 2: Create a stored procedure that runs the model to make predictions](#task-2-create-a-stored-procedure-that-runs-the-model-to-make-predictions)
    - [Lab B: Incorporating Hadoop Map/Reduce and Spark functionality into the ScaleR workflow](#lab-b-incorporating-hadoop-mapreduce-and-spark-functionality-into-the-scaler-workflow)
        - [Scenario](#scenario)
        - [Objectives](#objectives)
        - [Lab Setup](#lab-setup)
    - [Exercise 1: Using Pig with ScaleR functions](#exercise-1-using-pig-with-scaler-functions)
        - [Scenario](#scenario)
        - [Task 1: Examine the Pig script](#task-1-examine-the-pig-script)
        - [Task 2: Upload the Pig script and flight delay data to HDFS](#task-2-upload-the-pig-script-and-flight-delay-data-to-hdfs)
        - [Task 3: Run the Pig script and examine the results](#task-3-run-the-pig-script-and-examine-the-results)
        - [Task 4: Convert the results to XDF format and add field names](#task-4-convert-the-results-to-xdf-format-and-add-field-names)
        - [Task 5: Create graphs to visualize the airline delay data](#task-5-create-graphs-to-visualize-the-airline-delay-data)
        - [Task 6: Investigate the mean airline delay by route](#task-6-investigate-the-mean-airline-delay-by-route)
    - [Exercise 2: Integrating ScaleR code with Spark and Hive](#exercise-2-integrating-scaler-code-with-spark-and-hive)
        - [Scenario](#scenario)
        - [Task 1: Upload the sorted delay data to Hive](#task-1-upload-the-sorted-delay-data-to-hive)
        - [Task 2: Use Hive to perform ad-hoc queries over the data](#task-2-use-hive-to-perform-ad-hoc-queries-over-the-data)

## Lab A: Deploying a predictive model to SQL Server

### Scenario

You are developing a number of predictive models against various datasets to ascertain the most likely circumstances for flights being delayed. You want to store these models in a SQL Server database so that you can reuse them without having to rebuild them. You have also decided to relocate the data used by these models to SQL Server.

### Objectives

In this lab, you will:

- Upload the flight delay data to SQL Server and examine it.
- Fit a DForest model to the data to help predict flight delays.
- Save the model to SQL Server and create a stored procedure that enables a user to make predictions using this model.

### Lab Setup

- **Estimated Time**: 60 minutes
- **Username**: .\\student
- **Password**: Pa55w.rd

## Exercise 1: Upload the flight delay data

### Scenario

You have a copy of several samples of the flight delay data as XDF files. You want to upload this data to SQL Server where you can examine it more easily. In particular, you want to see whether delays caused by bad weather are predictable, based on the time of year.

The main tasks for this exercise are as follows:

1. Configure SQL Server and create the FlightDelays database
2. Upload the flight delay data to SQL Server
3. Examine the data in the database

### Task 1: Configure SQL Server and create the FlightDelays database

1. Log on to the **LON-DEV** VM as **.\\student** with the password **Pa55w.rd**.
2. Start **SQL Server Management Studio**. Log on to the server using Windows authentication.
3. Reconfigure SQL Server to enable external scripts.
4. Stop and restart SQL Server, and then create a new database named **FlightDelays**. Use the default options for this database.
5. Leave SQL Server Management Studio open.

### Task 2: Upload the flight delay data to SQL Server

1. Copy the FlightDelayDataSample.xdf and file from the **E:\\Labfiles\\Lab08** folder to the **E:\\Data** shared folder.
2. Start your R development environment of choice (Visual Studio, or RStudio), and create a new R file.
3. Ensure that you are running in the local compute context; this is because you cannot read or write XDF data in the SQL Server compute context.
4. Create an **RxSqlServerData** connection string and data source that connects to a table named **flightdelaydata** in the **FlightDelays** database. The database is located on the SQL server. You should use a trusted connection.
5. Use the **rxDataStep** function to upload the database in the **E:\\Data\\FlightDelayDataSample.xdf** file to the **flightdelaytable** in the SQL Server database. Add the following transformations:
    - Create an additional column named **DelayedByWeather**. This column should be a logical factor that is true if the **WeatherDelay** value in an observation is non-zero. For this exercise, treat **NA** values as zero.
    - Create another column called **Dataset**. You will use this column to divide the data into training and test datasets for the DForest model. The column should contain the text **"train"** or **"test"**, selected according to a random uniform distribution. Five percent of the data should be marked as **"test"** with the remainder labelled as **"train"**.

    > Note that the data file contains 1158143 (1.158 million) rows.

### Task 3: Examine the data in the database

1. Create a SQL Server compute context and use it to connect to SQL Server. You can reuse the connection string that you defined earlier, for creating the **RxSqlServerData** data source. Set the **wait** parameter of the compute context to **TRUE**.
2. Use the following Transact-SQL query to create another **RxSqlServerData** data source. This query retrieves the **Month**, **MonthName**, **OriginState**, **DestState**, **Dataset**, and **DelayedByWeather** columns, and also generates a calculated column named **WeatherDelayCategory** that partitions the data into categories according to the length of any weather delay:

    ```SQL
    SELECT Year, Month, MonthName, OriginState, DestState,
        DelayedByWeather, WeatherDelayCategory = CASE
            WHEN CEILING(WeatherDelay) \<= 0 THEN 'No delay'
            WHEN CEILING(WeatherDelay) BETWEEN 1 AND 30 THEN '1-30 minutes'
            WHEN CEILING(WeatherDelay) BETWEEN 31 AND 60 THEN '31-60 minutes'
            WHEN CEILING(WeatherDelay) BETWEEN 61 AND 120 THEN '61-120 minutes'
            WHEN CEILING(WeatherDelay) BETWEEN 121 AND 180 THEN '121-180 minutes'
            WHEN CEILING(WeatherDelay) \>= 181 THEN 'More than 180 minutes'
        END
    FROM flightdelaydata
    ```

    The **RxSqlServerData** data source should convert the following columns to factors:
    - **Month**
    - **OriginState**
    - **DestState**
    - **DelayedByWeather**
    - **WeatherDelayCategory**
    - **MonthName**

    The **Dataset** column should be **character**.

3. Run the **rxGetVarInfo** function over the data source and verify that it contains the correct variables. Note that the factors may be reported as having zero factor levels. This is fine as you have not yet retrieved any data, so the data source does not know what the factor levels are.
4. Summarize the data by using the **rxSummary** function. This might take a while as this is the point at which the data source reads the data from the database.
5. Create and display a histogram that shows the number delays in each value of **WeatherDelayCategory** conditioned by **MonthName**.
6. Create and display another histogram that shows the number delays in each value of **WeatherDelayCategory** conditioned by **OriginState**.

**Results**: At the end of this exercise, you will have imported the flight delay data to SQL Server and used ScaleR functions to examine this data.

**Question:** According to the first histogram, is there a pattern to weather delays?

**Question:** Using the second histogram, which states appear to have the most delays as a proportion of the flights that depart from airports in those states? Proportionally, which state has the fewest delays?

## Exercise 2: Fit a DForest model to the weather delay data

### Scenario

You surmise that weather delays might be related to the month, origin state, and possibly the destination state (by extension) for each flight. For accuracy, you decide to fit a decision tree forest to the data and score it against a different sample of the flight delay data. You will save the scored results in the database.

The main tasks for this exercise are as follows:

1. Create a DForest model
2. Score the DForest model

### Task 1: Create a DForest model

1. Create a DForest model that uses the following formula to fit the weather delay data:

    ```R
    DelayedByWeather ~ Month + OriginState + DestState
    ```

    - Only use the observations where the **Dataset** variable contains the value **"train"** to fit the model.
    - Specify a complexity parameter limit (cp) of 0.0001.

    >**Note:** Make sure you are still using the SQL Server compute context. It should take approximately five minutes to construct the model. However, if you are running in the local compute context, it can take more than 30 minutes to perform the same task. This is one advantage of keeping the computation close to the data, and exploiting the parallelism available with R Server rather than R Client.
    >Also note that you will receive a warning message stating that the **"Number of observations not available for this data source"** when the process completes. You can ignore this warning.

2. Inspect the model and examine the trees that it contains. Notice the forecast accuracy of the model based on the training data.
3. Use the **rxVarImpUsed** function to see the influence that each predictor variable has on the decisions made by the model.

**Question:** What is the Out-Of-Box (OOB) error rate for the DForest model?

**Question:** Are there any discrepancies between flights being forecast as delayed versus those being forecast as on-time? If so, how could you adjust for this?

**Question:** Which predictor variable had the most influence on the decisions made by the model?

### Task 2: Score the DForest model

1. Modify the data source that you used to retrieve the training data for the model, and limit it to fetch only test data. To do this, you will need to append a Transact-SQL **WHERE** clause to the **@sqlQuery** property of the data source.

    >**Note:** The **rxDForest** function has a **rowSelection** argument that you can use to limit the rows retrieved to those labeled as **"training"**. The **rxPredict** function that you will use to score does not have this capability, so you need to restrict the data by modifying the data source instead.

2. Create another **RxSqlServerData** data source. This data source should connect to a table named **scoredresults** in the **FlightDelays** database (this table doesn't exist yet).
3. Temporarily switch back to the local compute context and run the **rxPredict** function to make predictions about weather delays using the new dataset in SQL Server. Save the results in the **scoredresults** table. Include the model variables in the scored results. Specify a prediction type of **prob** to generate the probabilities of a match/nomatch for each case, and use the **preVarNames** agument to record these probabilities in columns named **PredictDelay** and **PredictNoDelay** in the **scoredresults** table. The **rxPredict** function also generates a TRUE/FALSE value that indicates, based on these probabilities, whether the flight will be delayed. Save this data in a column named **PredictedDelayedByWeather**.

    Note that the **rxPredict** function creates the **scoredresults** table.
    When the **rxPredict** function has finished, return to the SQL Server compute context.
    >**Note:** The **rxPredict** function does not currently work as expected in the SQL Server compute context, which is why you need to switch back to the local compute context.
4. Run the following code to test the accuracy of the weather delay predictions in the **scoredresults** table against the real data:

    ```R
    install.packages('ROCR')
    library(ROCR)

    # Transform the prediction data into a standardized form
    results <- rxImport(weatherDelayScoredResults)
    weatherDelayPredictions <- prediction(results$PredictedDelay, results$DelayedByWeather)

    # Plot the ROC curve of the predictions
    rocCurve <- performance(weatherDelayPredictions, measure = "tpr", x.measure = "fpr")
    plot(rocCurve)
    ```

    This code performs the following tasks:
    - It creates a data frame containing the scored results.
    - It uses the **prediction** function to compare the probability of a weather delay recorded in the **PredictedDelay** column of the scored results with the flag (TRUE=1, FALSE = 0) that indicates whether the flight was actually delayed by weather for each observation.
    - It runs the **performance** function to measure the ratio of true positive results against false positive results.
    - It plots the results as a ROC curve.

**Results**: At the end of this exercise, you will have created a decision tree forest using the weather data held in the SQL Server database, scored it, and stored the results back in the database.

**Question:** What does the ROC curve tell you about the possible accuracy of weather delay predictions? Is this what you expected?

## Exercise 3: Store the model in SQL Server

### Scenario

You want to save the model so that you can reuse it to make predictions against other datasets, for comparison purposes. You decide to store the model in SQL Server. You also decide to create a stored procedure that others can use to run predictions using your model.

The main tasks for this exercise are as follows:

1. Save the model to the database
2. Create a stored procedure that runs the model to make predictions

### Task 1: Save the model to the database

1. Serialize the DTree model as a string representation of binary data. Use the **serialize** and **paste** functions to do this.
2. Using **SQL Server Management Studio**, create the following table in the **FlightDelays** database. This table will hold the serialized model:

    ```SQL
    CREATE TABLE [dbo].[delaymodels]
    (
        modelId INT IDENTITY(1,1) NOT NULL Primary KEY,
        model VARBINARY(MAX) NOT NULL
    )
    ```

3. Add the following stored procedure. You can run this stored procedure from R to save the DTree model to the database:

    ```SQL
    CREATE PROCEDURE [dbo].[PersistModel] @m NVARCHAR(MAX)
    AS
    BEGIN
        SET NOCOUNT ON;
        INSERT INTO delaymodels(model) VALUES (CONVERT(VARBINARY(MAX),@m,2))
    END
    ```

4. In your R development environment, create an ODBC connection to the database and use the **sqlQuery** ODBC function to run the **PersistModel** stored procedure. Specify the serialized version of the DTree object as the parameter to the stored procedure.

    Note that the **sqlQuery** function is part of the **RODBC** library, and you must use the **odbcDriverConnect** function to create the ODBC connection. You can reuse the same connection string as before.

### Task 2: Create a stored procedure that runs the model to make predictions

1. In SQL Server Management Studio, create the following stored procedure:

    ```SQL
    CREATE PROCEDURE [dbo].[PredictWeatherDelay]
    @Month integer = 1,
    @OriginState char(2),
    @DestState char(2)
    AS
    BEGIN
        DECLARE @weatherDelayModel varbinary(max) = (SELECT TOP 1 model FROM dbo.delaymodels)
        EXEC sp\_execute\_external\_script @language = N'R', @script = N'
            delayParams \<- data.frame(Month = month, OriginState = originState, DestState = destState)
            delayModel \<- unserialize(as.raw(model))
            OutputDataSet\<-rxPredict(modelObject = delayModel,
            data = delayParams,
            outData = NULL,
            predVarNames = c("PredictedDelay", "PredictedNoDelay", "PredictedDelayedByWeather"),
            type = "prob",
           writeModelVars = TRUE)',
        @params = N'@model varbinary(max),
            @month integer,
            @originState char(2),
            @destState char(2)',
            @model = @weatherDelayModel,
            @month = @Month,
            @originState = @OriginState,
            @destState = @DestState
        WITH RESULT SETS (([PredictedDelay] float, [PredictedNoDelay] float, [PredictedDelayedByWeather] bit, [Month] integer, [OriginState] char(2), [DestState] char(2)));
    END
    ```

    This stored procedure takes three input parameters: **Month**, **OriginState**, and **DestState**. These parameters equate to the predictor variables used by the DTree model.

    The body of the stored procedure creates a variable named **@weatherDelayModel** that will be used to retrieve the model from the **delaymodels** table.

    The remainder of the stored procedure uses the **sp\_execute\_external\_script** stored procedure to run a chunk of R code. This R code takes four parameters: **@model** which is used to reference the model to run, and **@month**, **@originState**, and **@destState** which specify the predictor values passed in. The assignments below the **@params** definition shows how these parameters are populated using the **@weatherDelayModel**, **@Month**, **@OriginState**, and **@DestState** variables respectively.

    The R code uses these variables to construct a data frame containing predictor values and also to retrieve the model from the database. The **rxPredict** function in the R code generates a prediction from this data indicating whether a flight from the specified origin to destination in the given month is likely to be delayed by weather. The results are output as another data frame (containing a single row). The **WITH RESULT SETS** clause specifies the fields in this data frame.

2. Return to your R development environment and run the following code to test the stored procedure:

    ```R
    cmd <- "EXEC [dbo].[PredictWeatherDelay] @Month = 11, @OriginState = 'GA', @DestState = 'NY'"
    sqlQuery(connection, cmd)
    ```

    This code asks about the probability of a flight from Michigan to New York in October being delayed due to weather.

3. Save the script as **Lab8\_1Script.R** in the **E:\\Labfiles\\Lab08** folder, and close your R development environment.
4. Close SQL Server Management Studio, without saving any changes.

**Results**: At the end of this exercise, you will have saved the DForest model to SQL Server, and created a stored procedure that you can use to make weather delay predictions using this model.

**Question:** According to the DForest model, what is the probability of a flight from Georgia (GA) to New York (NY) in November being delayed by weather? What about a flight in June?

## Lab B: Incorporating Hadoop Map/Reduce and Spark functionality into the ScaleR workflow

### Scenario

You want to analyze flights caused by airline delays rather than bad weather. Another Hadoop developer has created a Pig script that captures this information, and you want to examine and process the data using R. You want to save the results of the processing to a Hive database to enable other developers to perform their own ad-hoc queries. Finally, you want to extract specific information from this data to list the delays for a specific airline.

### Objectives

In this lab, you will:

- Use R code in the Hadoop Map/Reduce compute context to run a Pig script that generates data and use ScaleR functions to examine the results.
- Use ScaleR functions to store data in a Hive database.
- Run sparklyr code in the Spark compute context to retrieve data from the Hive database, and filter this data.

### Lab Setup

- **Estimated Time**: 30 minutes
- **Username**: .\\student
- **Password**: Pa55w.rd

## Exercise 1: Using Pig with ScaleR functions

### Scenario

The Pig script summarizes flight delay data held in CSV format, and generates a dataset that contains the origin airport, destination, airline code, and the duration of any delays due to the airline (carrier delays and late aircraft delays). This script also retrieves the airline name from another CSV file which it joins with the flight delay data. You want to generate some charts that illustrate how the different airlines compare for delays.

The main tasks for this exercise are as follows:

1. Examine the Pig script
2. Upload the Pig script and flight delay data to HDFS
3. Run the Pig script and examine the results
4. Convert the results to XDF format and add field names
5. Create graphs to visualize the airline delay data
6. Investigate the mean airline delay by route

### Preparation

1. Ensure that the VM machines are running.
2. Follow the instructions in the document **Creating the Hadoop Cluster on Azure** to deploy and configure an Azure HDInsight cluster.
3. In the Azure portal, go to the **LON-HADOOP-SERVER** resource group, and click the **LON-HADOOP-*nn*** HDInisght cluster.
4. On the **LON-HADOOP-*nn*** blade, in the **R Server dashboards** section, click **Ambari home**.
5. In the **Sign in** dialog box, sign in as **admin** with password **Pa55w.rdPa55word**.
6. On the **Ambari** page, in the left pane, click **Tez**.
7. Click the **Configs** tab.
8. Expand the **Advanced tez-site** section.
9. Change the value of the **tez.runtime.io.sort.mb** configuration setting to **1024 MB**.
10. Click **Save**.
11. In the **Save Configuration** dialog box, click **Save**.
12. In the **Configurations** dialog box, click **Proceed Anyway**.
13. In the **Save Configuration Changes** dialog box, click **OK**.
14. Click **Restart**, and then click **Restart All Affected**.
15. In the **Confirmation** dialog box, click **Confirm Restart All**.
16. Wait for the Tez components to restart, and then click **OK**.

### Task 1: Examine the Pig script

1. Log on to the **LON-DEV** VM as **.\\student** with the password **Pa55w.rd**.
2. Using **WordPad**, open the file **carrierDelays.pig** in the **E:\\Labfiles\\Lab08** folder. This file is the Pig script that you will run. This script performs the following tasks:

   - It reads a CSV file named **carriers.csv** held in the /user/RevoShare/*loginName* directory in HDFS (where *loginName* is your login name). This file contains airline codes and their names.
   - It reads another file called **FlightDelayDataSample.csv** which contains flight delay information. This file contains a subset of the information about flight delays that you have been using throughout the course.
   - It filters the flight delay information to find all delays that have a positive value in the **LateAircraftDelay** or **CarrierDelay** fields. These fields indicate delays that are caused by the airline.
   - It joins this data with the airline name in the **carriers.csv** file.
   - It writes a dataset containing the origin airport, destination, airline code, airline name, carrier delay, and late aircraft delay fields to a file in Pig storage (located in the /user/RevoShare/*loginName*/results directory in HDFS).

3. Edit the first line of the script, and change the text **{specify login name}** to your login name, as shown in the following example:

    ```TEXT
    %declare loginName 'student01'
    ```

4. Save the file and close Wordpad.

### Task 2: Upload the Pig script and flight delay data to HDFS

1. Using **File Explorer**, go to the folder **E:\\Labfiles\\Lab08**, right-click the file **FlightDelayDataSample.zip**, and then click **Extract All**.
2. In the **Extract Compressed (Zipped) Folders** dialog box, specify the folder **E:\\Labfiles\\Lab08**, and then click **Extract**. This file contains the flight delay sample data in CSV format.
3. Start your R development environment of choice (Visual Studio, or RStudio), and create a new R file.
4. Create an **RxHadoopMR** compute context. Specify a login name for the **sshUsernme** argument, set the **sshHostname** to **"LON-HADOOP"**, and set the **consoleOutput** argument to **TRUE** (this is so you can view the messages that Hadoop displays).
5. Use the **rxHadoopCopyFromClient** function to copy the **E:\\Labfiles\\Lab08\\FlightDelayDataSample.csv** file to the **/user/RevoShare/sshuser** directory in HDFS. This is a large file, and it may take a couple of minutes to upload.

    >**Note:** You might also want to remove any existing file with the same name in the HDFS folder first. You can use the **rxHadoopRemove** function to do this.

6. Copy the **E:\\Labfiles\\Lab08\\carriers.csv** file to the **/user/RevoShare/sshuser** directory in HDFS.
7. Close the **RxHadoopMR** compute context, and use the **remoteLogin** function to create a remote connection to R Server running on the Hadoop cluster. The **deployr\_endpoint** argument should be **http://*ipaddress*:12800**, where *ipaddress* is the public IP address of the edge node of the cluster. The username is **admin**, and the password is **Pa55w.rd**.

    > **Note**: You can find the public IP address of the edge node using the following steps:
    > 1. Switch to the Azure portal.
    > 2. On the navigation blade on the left side of the portal, click **Resource groups**.
    > 3. Click the **LON-HADOOP-SERVER** resource group.
    > 4. On the **LON-HADOOP-SERVER** blade, click the **ipConfig1** Public IP address.
    > 5. On the **ipConfig1** blade, make a note of the IP address.

8. Add the following statements to the R file and run them. Replace the text **ipaddress** with value of the **ipConfig1** public IP address for the edge node of the cluster.
9. Load the **RevoScaleR** library in the remote session.
10. Pause the remote session, use the **putLocalFile** function to copy the **carrierDelays.pig** file to the remote session, and then resume the remote session.

### Task 3: Run the Pig script and examine the results

1. In the remote session, use the **system** function to run the Pig script as follows:

    ```R
    result <- system("pig carrierDelays.pig", intern = TRUE)
    ```

    If the command fails, you can examine the reason code for the failure in the **result** variable.

2. The Pig script saves the output to a directory named **results** in HDFS. Use the following code to verify that this directory has been created.:

    ```R
    rxHadoopCommand("fs -ls -R /user/RevoShare/sshuser", intern = TRUE)
    ```

3. Verify that the results directory contains two files: **\_SUCCESS** and **part-r-00000**. The data is actually in the part-r-00000 file. The **\_SUCCESS** file should be empty; it is simply a flag created by the Pig script to indicate that the data was successfully saved.

4. Use the **rxHadoopRemove** function to delete the **\_SUCCESS** file from the **results** folder.
5. Use the **rxGetVarInfo** function to examine the structure of the data in the **results** directory. To do this, switch to the HDFS file system, and create an **RxTextData** data source that references the **results** directory (not the part-r-00000 file). Note that the data does not include any useful field names in the schema information.

### Task 4: Convert the results to XDF format and add field names

1. Create a **colInfo** list that can be used by the **rxImport** function to add schema information to a file as it is imported. Map the fields in the existing results file as follows:

    - **V1**: type = "factor", nameName = "Origin"
    - **V2** type = "factor", nameName = "Dest"
    - **V3**: type = "factor", nameName = "AirlineCode"
    - **V4**: type = "character", nameName = "AirlineName"
    - **V6**: type = "numeric", nameName = "CarrierDelay"
    - **V7**: type = "numeric", nameName = "LateAircraftDelay"

2. Use the **rxImport** function to create an XDF file from the results data using the column information mapping you just defined. Save the XDF file as the **CarrierData** composite XDF file in your directory under **/user/RevoShare** in HDFS. The new file should contain 120,827 rows.
3. While in the remote session, create a new **RxHadoopMR** compute context. Note that you do not need to specify a host name as the context will run on the same computer as the remote session.
4. Run the **rxGetVarInfo** and **rxSummary** functions to verify the new mappings in the **CarrierData** XDF file. Note that the **rxSummary** function is run as a Map/Reduce job.

### Task 5: Create graphs to visualize the airline delay data

1. Generate a histogram that shows the duration of airline delays on the X axis and the number of times these delays occur on the Y axis. The duration of an airline delay is the sum of the carrier delay and late aircraft delay fields for each observation. Limit the delays reported to those of 300 minutes or less. Note that the **RxHadoopMR** function does not support transformations as part of the rxHistogram function, so you should create a temporary data frame that contains the transformed data first, and use this data frame to construct the histogram.
2. Generate another histogram that shows the number of delayed flights by airline code. Use the XDF file rather than the data frame for this graph. Note that you might need to increase the resolution of the plot window to display the codes for all airlines. You can do this using the **png** function. However, you must execute the **png** function and the **rxHistogram** function together and not as separate commands in a remote session.
3. Generate a bar chart showing the total delay time across all flights for each airline. Display the airline name rather than the airline code. Note that this graph requires you to use **ggplot** with the **geom\_bar** function rather than **rxHistogram**.

### Task 6: Investigate the mean airline delay by route

1. Create a data cube that calculates the mean delay for each airline by route (a route is a combination of the origin and destination):

    ```R
    delayData <- rxCube(AverageDelay ~ AirlineCode:Origin:Dest, carrierData,
        transforms = list(AverageDelay = CarrierDelay + LateAircraftDelay),
        means = TRUE,
        na.rm = TRUE,
        removeZeroCounts = TRUE)
    ```
2. Use the **rxSort** function to sort the cube in descending order of the **Counts** and **AverageDelay** variables in the cube. Note that the **rxSort** function is not inherently distributable, so you cannot use it directly in the **RxHadoopMR** environment. However, you can use the **rxExec** function to run it if you set the **timesToRun** argument of **rxExec** to 1.
3. Display the top 50 (the worst routes for delays), and the bottom 50 (the best routes). Note that the value returned by **rxExec** is a list of results named **rxElem1**, **rxElem2**, and so on; one item for each task performed in parallel. There was only one task (**timesToRun** was set to 1), so you can find all the data in the **rxElem1** field.
4. Save the sorted data cube to the CSV file **SortedDelayData.csv** in your directory under the /user/RevoShare directory in HDFS. Perform this operation using the local compute context rather than Hadoop. The file should contain 8,852 rows.

**Results**: At the end of this exercise, you will have run a Pig script from R, and analyzed the data that the script produces.

**Question:** According to the histogram that displays delays against frequency, what is the most common delay period across all airlines?

**Question:** Using the second histogram, which airline has had the most delayed flights?

**Question:** Which route has the most frequent airline delays? How long is the average airline delay on this route?

## Exercise 2: Integrating ScaleR code with Spark and Hive

### Scenario

You want to upload the sorted delay data to Hive to enable other users to examine it in a more ad-hoc manner.

The main tasks for this exercise are as follows:

1. Upload the sorted delay data to Hive
2. Use Hive to perform ad-hoc queries over the data

### Task 1: Upload the sorted delay data to Hive

1. In your remote R session, create an **RxSpark** compute context with the following parameters:
    - **sshUsername**: your login name
    - **consoleOutput:** TRUE
    - **numExecutors**: 10
    - **executorCores**: 2
    - **executorMem** : "1g"
    - **driverMem** = "1g"

    Note that it is important to set the resource parameters of the **RxSpark** session appropriately, otherwise you risk grabbing all the resources available and starving other concurrent users of the cluster.

2. Upload the data to a table named **RouteDelays** in Hive. Create an **RxHiveData** data source that you can use to reference this table.
3. Use the **RxDataStep** function to upload the data. The **inData** argument should reference the CSV file containing the sorted data cube, and the **outFile** argument should specify the Hive data source.
4. Use the **rxSummary** function over the Hive data source to verify that the data was uploaded successfully.

### Task 2: Use Hive to perform ad-hoc queries over the data

1. Open a command prompt window, and run the **putty** command.
2. In the **PuTTY Configuration** window, select the **LON-HADOOP** session, click **Load**, and then click **Open**. You should be logged in to the Hadoop VM.
3. In the PuTTY terminal window, run the following command:

    ```Bash
    /usr/bin/hive
    ```

4. At the **hive\>** prompt, run the following command:

    ``` SQL
    select * from sshuserroutedelays;
    ```

    Verify that 8852 rows are retrieved.

5. At the **hive\>** prompt, run the following command:

    ```SQL
    select count(*), airlinecode from sshuserroutedelays group by airlinecode;
    ```

    This command lists each airline together with the number of delayed flights for that airline.

6. At the **hive\>** prompt, run the following command to exit hive:

    ```SQL
    exit;
    ```

7. Close the PuTTY terminal window, and return to your R development environment.
8. Save the script as **Lab8\_2Script.R** in the **E:\\Labfiles\\Lab08** folder, and close your R development environment.

**Results**: At the end of this exercise, you will have used R code running in an RxSpark compute context to upload data to Hive, and then analyzed the data by performing Hive queries..

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
