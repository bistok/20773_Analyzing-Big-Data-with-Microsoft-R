# Module 7: Creating and Evaluating Partitioning Models

- [Module 7: Creating and Evaluating Partitioning Models](#module-7-creating-and-evaluating-partitioning-models)
    - [Demo 1: Building Partitioning Models](#demo-1-building-partitioning-models)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Categorize the Data](#categorize-the-data)
        - [Fit Models over the Training Data](#fit-models-over-the-training-data)
        - [Prune the decision tree to reduce complexity](#prune-the-decision-tree-to-reduce-complexity)
    - [Demo 2: Running Predictions against Partitioning Models](#demo-2-running-predictions-against-partitioning-models)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Run predictions and assess the results](#run-predictions-and-assess-the-results)
        - [Visualize the models](#visualize-the-models)

## Demo 1: Building Partitioning Models

### Scenario

In this demonstration, you will see how to build a partitioning model that can be used to predict the value of a diamond given its cut, clarity, carats, and color, based on the diamonds dataset. The dataset is initially categorized into diamonds of high value and diamonds of low value. High value diamonds are those with a price of $4,000 or more. The dataset is then trimmed so that only the cut, clarity, carat, and color variables remain. This demonstration fits three models to the data: a DTree, a DForest, and a BTree.

### Preparation

1. Start the **20773A-LON-DC**, **20773A-LON-DEV**, **20773A-LON-SQLR**, **20773A-LON-RSVR**, and **MT17B-WS2016-NAT** VMs if they are not already running. 
2. Log in to the **20773A-LON-DEV** VM as **Adatum\AdatumAdmin** with the password **Pa55w.rd**.

### Categorize the Data

1. Open your R development environment of choice (RStudio or Visual Studio 2017®).
2. Open the R script **Demo1 - partitioning.R** in the **E:\\Demofiles\\Mod07** folder.
3. Highlight and run the code under the comment **# Examine the diamond data**. This code displays the first 20 rows in the diamonds dataset. Notice that each diamond has variables that include the number of carats, the cut, the color, and the clarity, together with other attributes that concern the geometry of a diamond.
4. Highlight and run the code under the comment **# Generate a dataset containing just the columns required**. This code uses the **rxDataStep** function to:
    - Add a factor variable named value with the values high and low. Diamonds are categorized according to their price.
    - Add a factor variable named set with the values train and test. Note that 95 percent of the data is selected at random and placed into the train category; the remainder is placed in the test category.
    - Remove variables not required by the model, leaving only cut, clarity, carat, and color.

5. Highlight and run the code under the comment **# Divide the dataset into training and test data**. This code uses the **rxSplit** function to separate out the observations in the dataset according to the value of the **set** category. The result is two datasets named **Diamonds.set.test.xdf** and **Diamonds.set.train.xdf**.

### Fit Models over the Training Data

1. Highlight and run the code under the comment **# Fit a DTree model**. This code uses the **rxDTree** function to fit the model to the training data. The formula specifies that the value depends on the cut, carat, color, and clarity of a diamond. The tree has a maximum node depth of four levels. 
2. Highlight and run the code under the comment **# Show the results**. This code displays the model fit. The top level node summarizes the split into high and low value diamonds. The subsequent nodes show how the decision to classify diamonds was made, based on the other variables.
3. Highlight and run the code under the comment **# For comparision, fit a DTreeForest model**. This code uses the **rxDForest** function to fit the model to the training data. The forest generates 50 decision trees and takes significantly longer to run in consequence. The results display how the data in the value field of each diamond compares to that generated by the model.
4. Highlight and run the code under the comment **# ... and a BTree model**. This code uses the **rxBTrees** function to fit the model to the training data. Again, this model generates 50 trees and takes a while to run. The results don't include specific details of the categorization, but rather display the error rate for the recorded value of a diamond compared to its assessed value, in terms of the deviance.

### Prune the decision tree to reduce complexity

1. Highlight and run the code under the comment **# Assess the complexity of the DTree model**. This generates a complexity plot of the model that is displayed as a scree plot. The point at which the plot flattens out shows where increasing the complexity adds little value, so you should remove these elements from the decision tree to retain efficiency.
2. Highlight and run the code under the comment **# Prune the tree to remove unnecessary complexity from the model**. This code removes the least important splits from the tree, based on the value selected from the scree plot.
3. Close the script without saving changes, but leave your R development environment open.

## Demo 2: Running Predictions against Partitioning Models

### Scenario

In this demonstration, you will see how to use the partitioning models created in the previous demonstration to predict the value of diamonds.

### Preparation

1. Start the **20773A-LON-DC**, **20773A-LON-DEV**, **20773A-LON-SQLR**, **20773A-LON-RSVR**, and **MT17B-WS2016-NAT** VMs if they are not already running.
2. Log in to the **20773A-LON-DEV** VM as **Adatum\AdatumAdmin** with the password **Pa55w.rd**.

### Run predictions and assess the results

1. In your R development environment, open the R script **Demo2 - predicting.R** in the **E:\\Demofiles\\Mod07** folder.
2. Highlight and run the code under the comment **# Create a copy of the test set without the value and set variables**. This code creates an in-memory data frame containing the data from the test set but with the **value** and **set** variables removed.
3. Highlight and run the code under the comment **# Predict the value of each diamond in the test set using the DTree model**. This code uses the DTree model to predict the value of each diamond in the test set. The results are stored in an **rxPredict** object with a variable, named **Value_Pred**, for each row in the test set. This variable will contain the value **high** or **low**. Note that, if you want to see the probability of each decision, you can omit the type argument from the **rxPredict** function. You can also compute residuals if required.
4. Highlight and run the code under the comment **# Assess the results against the values recorded in the test set**. This code generates a summary of the split between **high** and **low** values in the test data, and in the predicted results, for comparison. The predicted number of **high** and **low** values should be within a few percent of the **actuals**.
5. Highlight and run the code under the comment **# Repeat using the DForest model**. This code uses the **DForest** model to generate predictions using the test dataset, and assesses the accuracy of the results. Ideally, these results should be closer to the actual values than those predicted by the DTree model.
6. Highlight and run the code under the comment **# Add the predicted value of each diamond to the in-memory data frame**. This code merges the test data used to generate the predictions with the predicted values for each diamond.
7. Highlight and run the code under the comment **# Compare the predicted results against the actual values by variable**. This code generates summaries of the original test data and the data frame with the predicted results. You can browse this data to determine which factors lead to discrepancies.

### Visualize the models

1. Highlight and run the code under the comment **# Visualize the DTree model using RevoTreeView**. This code uses the **createTreeView** function to display a representation of the tree in a web browser. You can click the nodes in the tree to see how the classification of diamonds was made based on the variables. Close the browser when you have finished.
2. Highlight and run the code under the comment **# Generate a line plot of the DTree model**. This code uses the **rpart** library to display a line plot of the tree. The structure of the plot should mirror that shown by using RevoTreeView.
3. Highlight and run the code under the comment **# Show an importance plot from the DForest model**. This code generates a dotchart showing the importance of each variable in classifying diamonds as used by the DForest model. The **carat** variable is clearly the most significant.
4. Close your R development environment with saving any changes.

---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
